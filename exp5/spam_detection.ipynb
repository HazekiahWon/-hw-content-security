{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'datasets\\spam_100.utf8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load stop words from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file):\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        stop_words = [x.strip('\\n') for x in f.readlines()]\n",
    "        stop_words = set(stop_words)\n",
    "    return stop_words\n",
    "stop_words = load_stopwords(r'stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '老',\n",
       " '累次',\n",
       " '除非',\n",
       " '和',\n",
       " '不妨',\n",
       " '背地里',\n",
       " '哪样',\n",
       " '不',\n",
       " '才能',\n",
       " '乘势',\n",
       " '归根结底',\n",
       " '出去',\n",
       " '其中',\n",
       " '过于',\n",
       " '风雨无阻',\n",
       " '虽',\n",
       " '顺',\n",
       " '纵',\n",
       " '不独']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stop_words)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data while deleting punctuations using re.sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(data_path, encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        lines.append(pattern.sub('', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'本公司有部分普通发票商品销售发票增值税发票及海关代征增值税专用缴款书及其它服务行业发票公路内河运输发票可以以低税率为贵公司代开本公司具有内外贸生意实力保证我司开具的票据的真实性希望可以合作共同发展敬侯您的来电洽谈咨询联系人：李先生联系电话：13632588281如有打扰望谅解祝商琪'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize the emails with `jieba` and filter stop words out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\hasee\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.714 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "tokenized = [[x for x in jieba.cut(line,cut_all=True) if not x in stop_words] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['公司', '部分', '普通', '普通发票', '发票', '商品', '商品销售', '销售', '发票', '增值']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the tokens to our corpus with `gensim.corpora`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use this method to merge other corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method merge_with in module gensim.corpora.dictionary:\n",
      "\n",
      "merge_with(other) method of gensim.corpora.dictionary.Dictionary instance\n",
      "    Merge another dictionary into this dictionary, mapping same tokens to the same ids and new tokens to new ids.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The purpose is to merge two corpora created using two different dictionaries: `self` and `other`.\n",
      "    `other` can be any id=>word mapping (a dict, a Dictionary object, ...).\n",
      "    \n",
      "    Get a transformation object which, when accessed as `result[doc_from_other_corpus]`, will convert documents\n",
      "    from a corpus built using the `other` dictionary into a document using the new, merged dictionary.\n",
      "    \n",
      "    Warnings\n",
      "    --------\n",
      "    This method will change `self` dictionary.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      "        Other dictionary.\n",
      "    \n",
      "    Return\n",
      "    ------\n",
      "    :class:`gensim.models.VocabTransform`\n",
      "        Transformation object.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from gensim.corpora import Dictionary\n",
      "    >>>\n",
      "    >>> corpus_1, corpus_2 = [[\"a\", \"b\", \"c\"]], [[\"a\", \"f\", \"f\"]]\n",
      "    >>> dct_1, dct_2 = Dictionary(corpus_1), Dictionary(corpus_2)\n",
      "    >>> dct_1.doc2bow(corpus_2[0])\n",
      "    [(0, 1)]\n",
      "    >>> transformer = dct_1.merge_with(dct_2)\n",
      "    >>> dct_1.doc2bow(corpus_2[0])\n",
      "    [(0, 1), (3, 2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dictionary.merge_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building onehot vector for each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bows = [dictionary.doc2bow(email) for email in tokenized]\n",
    "one_hot = lambda x:[1 if k in x else 0 for k in dictionary.values()]\n",
    "one_hot(tokenized[0])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the corresponding section in my integrated code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics\\rd.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing for following operations, calling `prepare_onehot()`  \n",
    "this method generates a one hot vector for each email following the preceding experiment.\n",
    "    1. tokenize each email\n",
    "    2. add tokens to the corpus\n",
    "    3. do the preceding for both spams and hams\n",
    "    4. obtain one hot vector via the corpus for each tokenized email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics\\onehot.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this method is exactly the same as the preceding experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics\\token.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 implementing Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important** \n",
    "I'm reusing my previous codes written for the `Pattern Recognition` class,  \n",
    "You can check it [here](https://github.com/HazekiahWon/-hw-pattern-recognition.git) in my another repo for the homework of `Pattern Recognition`  \n",
    "以下代码来自我以前写的模式识别的贝叶斯作业，仅做必要说明，因为这份代码不限于朴素贝叶斯，包括多元高斯的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我仅继承了NaiveBayes类，支持很多选项，比如交叉验证和PCA，但这里并未使用\n",
    "<img src='pics\\init.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training\n",
    "第一个核心函数是`train()`，用来计算先验，即类概率和属性的类概率密度\n",
    "<img src='pics\\train.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_compute_class_prior()`计算类概率\n",
    "<img src='pics\\cp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_compute_class_pd_prior()`计算类概率密度, 简单说就是通过统计频数近似概率\n",
    "<img src='pics\\pdp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing\n",
    "`test()`核心是这几行，附带了很多validation和accuracy的辅助语句\n",
    "<img src='pics\\test.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中`_predict()`就是求样本属于每个类的概率再求最大值\n",
    "<img src='pics\\pred.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后求某类的条件概率由`_compute_class_proba()`完成，利用朴素假设做连乘，但实际上取log，所以加法\n",
    "<img src='pics\\proba.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Results\n",
    "please check the output file `logger.txt` for details.  \n",
    "In the validation phase, i got 99.5% accuracy  \n",
    "And the results on the testing data seems satisfactory\n",
    "<img src='pics\\log.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
