{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬一个小说网站\n",
    "用scrapy shell做实验，首先到一个类似书目列表的页面，是按照类别分的  \n",
    "<img src='pics\\1.png'>\n",
    "\n",
    "从中选一个分区，进一步提取其中的每一本书，并进入该书籍的目录页面  \n",
    "提取出链接后跳转  \n",
    "一开始在解析某本书的目录页时遇到一个小问题：  \n",
    "由于目录最前面会有一个最新章节的格子，里面的章节和后面的有重复  \n",
    "所以要定位到后面正文的格子里\n",
    "<img src='pics\\2.png'>\n",
    "\n",
    "用了xpath语法`following-sibling::`\n",
    "<img src='pics\\3.png'>\n",
    "\n",
    "跳转到小说的某章节页面，再后面提取小说正文就很简单了  \n",
    "<img src='pics\\4.png'>\n",
    "\n",
    "更详细地请见`novels`的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "上一步的爬取结果是以书名为单位保存的，请见`save`目录   \n",
    "首先遍历每本小说，分别使用`jieba`分词，并去掉停用词  \n",
    "<img src='pics\\5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中的一个细节是，在分词前，我直接利用`re`去掉了非中文字符  \n",
    "另外，去除停用词我利用了差集，并使用`sorted`的`key`保留原始的分词顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "按照pdf做的  \n",
    "<img src='pics\\6.png'>  \n",
    "模型文件保存为`novels.model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "我目前提交的全部是`修仙玄幻武侠`分区的部分小说，所以使用了相关的词语实验  \n",
    "<img src='pics\\7.png'>  \n",
    "\n",
    "p.s. `金丹`在仙侠小说中是一种常见力量体系的阶段，与之并列的包括`练气筑基元婴化神`  \n",
    "<img src='pics\\8.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
