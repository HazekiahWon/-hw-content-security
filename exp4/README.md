# for TA
all reports are written in jupyter notebooks  
in this experiment:  
    1. `novels/`  
        a scrapy project, the spider for crawling some novels  
    2. `tokenization.py`  
        a tokenizer for w2v
# notebooks
- tokenization.ipynb  
the report for this experiment

# results
- `novels/save/`   
    the directory storing my corpus - crawled from `www.booktxt.com`  
    however it's too big so i rar it
- `agg.txt`  
    the aggregation file for the tokenization results of different novels  
    however it's too big so i rar it
- `novels.model`  
    the w2v model trained using the preceding file